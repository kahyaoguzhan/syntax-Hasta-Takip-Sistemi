"""
Gemini API Handler
En yeni prompt dosyasÄ±nÄ± bulup Gemini API'ye gÃ¶nderen modÃ¼l
"""

import os
import glob
from datetime import datetime
from PyQt6.QtCore import QThread, pyqtSignal
import google.generativeai as genai


def get_latest_analysis_json(directory="analysis_results"):
    """
    En yeni analysis_result_*.json dosyasÄ±nÄ± bulur
    
    Args:
        directory: JSON dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r
        
    Returns:
        str: En yeni dosyanÄ±n yolu veya None
    """
    try:
        # analysis_result_*.json dosyalarÄ±nÄ± bul
        pattern = os.path.join(directory, "analysis_result_*.json")
        files = glob.glob(pattern)
        
        if not files:
            return None
        
        # En yeni dosyayÄ± bul (modification time'a gÃ¶re)
        latest_file = max(files, key=os.path.getmtime)
        return latest_file
        
    except Exception as e:
        print(f"JSON dosyasÄ± bulma hatasÄ±: {e}")
        return None


def create_prompt_from_json(json_filepath):
    """
    JSON dosyasÄ±ndan prompt metni oluÅŸturur
    
    Args:
        json_filepath: JSON dosyasÄ±nÄ±n yolu
        
    Returns:
        str: OluÅŸturulan prompt metni veya None
    """
    try:
        import json
        from signal_processor import create_prompt_from_results
        
        # JSON dosyasÄ±nÄ± oku
        with open(json_filepath, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # Prompt oluÅŸtur
        prompt = create_prompt_from_results(results)
        return prompt
        
    except Exception as e:
        print(f"Prompt oluÅŸturma hatasÄ±: {e}")
        return None


def get_all_analysis_json_files(directory="analysis_results"):
    """
    TÃ¼m analysis_result_*.json dosyalarÄ±nÄ± bulur ve tarihe gÃ¶re sÄ±ralar
    
    Args:
        directory: JSON dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r
        
    Returns:
        list: SÄ±ralanmÄ±ÅŸ dosya yollarÄ± listesi (en eskiden en yeniye)
    """
    try:
        # analysis_result_*.json dosyalarÄ±nÄ± bul
        pattern = os.path.join(directory, "analysis_result_*.json")
        files = glob.glob(pattern)
        
        if not files:
            return []
        
        # Tarihe gÃ¶re sÄ±rala (modification time - en eskiden en yeniye)
        files_sorted = sorted(files, key=os.path.getmtime)
        return files_sorted
        
    except Exception as e:
        print(f"JSON dosyalarÄ± bulma hatasÄ±: {e}")
        return []


class GeminiWorker(QThread):
    """
    Gemini API Ã§aÄŸrÄ±sÄ±nÄ± arka planda yapan worker thread
    """
    # Signaller
    status_update = pyqtSignal(str)  # Durum gÃ¼ncellemesi
    chunk_received = pyqtSignal(str)  # YanÄ±t chunk'Ä± geldi
    completed = pyqtSignal()  # Ä°ÅŸlem tamamlandÄ±
    error_occurred = pyqtSignal(str)  # Hata oluÅŸtu
    
    def __init__(self, prompt_text):
        super().__init__()
        self.prompt_text = prompt_text
        self.should_stop = False
        
    def run(self):
        """Thread ana fonksiyonu"""
        try:
            # API Key kontrolÃ¼
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                self.error_occurred.emit(
                    "HATA: GEMINI_API_KEY Ã§evre deÄŸiÅŸkeni bulunamadÄ±.\n"
                    "LÃ¼tfen API anahtarÄ±nÄ±zÄ± ayarlayÄ±n:\n"
                    "$env:GEMINI_API_KEY='API_ANAHTARINIZ' (PowerShell)"
                )
                return
            
            # Gemini API konfigÃ¼rasyonu
            self.status_update.emit("API konfigÃ¼rasyonu yapÄ±lÄ±yor...")
            genai.configure(api_key=api_key)
            
            # Model ayarlarÄ± - Daha uzun ve detaylÄ± yanÄ±tlar iÃ§in
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 8192,
            }
            
            # GÃ¼venlik ayarlarÄ± - TÄ±bbi iÃ§erik iÃ§in
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE",
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE",
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE",
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE",
                },
            ]
            
            # Model oluÅŸtur
            self.status_update.emit("Model yÃ¼kleniyor...")
            model_name = "gemini-2.5-flash"
            model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            # Prompt'u terminale yazdÄ±r
            print("-" * 80)
            print("ğŸ“¤ GEMINI'YE GÃ–NDERÄ°LEN PROMPT:")
            print("-" * 80)
            print(self.prompt_text)
            print("-" * 80)
            print("\n")
            
            # Streaming ile yanÄ±t al
            self.status_update.emit("Gemini'den yanÄ±t bekleniyor...")
            print("ğŸ“¥ GEMINI'DEN GELEN YANIT:")
            print("-" * 80)
            response = model.generate_content(self.prompt_text, stream=True)
            
            full_response = ""
            chunk_count = 0
            
            for chunk in response:
                if self.should_stop:
                    break
                    
                if chunk.text:
                    # UI'a gÃ¶nder
                    self.chunk_received.emit(chunk.text)
                    # Terminale yazdÄ±r
                    print(chunk.text, end="", flush=True)
                    full_response += chunk.text
                    chunk_count += 1
            
            # TamamlandÄ±
            print("\n" + "-" * 80)
            if not self.should_stop:
                print(f"âœ… TamamlandÄ±! {chunk_count} chunk, {len(full_response)} karakter")
                self.status_update.emit(
                    f"TamamlandÄ±! {chunk_count} chunk, {len(full_response)} karakter"
                )
                self.completed.emit()
            
            # YanÄ±t Ã§ok kÄ±saysa uyarÄ±
            if len(full_response) < 500:
                self.error_occurred.emit(
                    "âš ï¸ UYARI: YanÄ±t beklenenden Ã§ok kÄ±sa! "
                    "Prompt safety filter'a takÄ±lmÄ±ÅŸ olabilir."
                )
                
        except Exception as e:
            self.error_occurred.emit(f"Hata oluÅŸtu: {str(e)}")
    
    def stop(self):
        """Worker'Ä± durdur"""
        self.should_stop = True
